# -*- coding: utf-8 -*-
import scipy.sparse as sparse
import numpy
from sklearn.metrics.pairwise import cosine_similarity
from MemNN.input import g_q_single_question

stopwords = {
    'english': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn'],
    'french': ['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'je', 'la', 'le', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']
}

def ngrams(words, n):
    """

    Note:
        generate ngrams of certain length with a list of word

    Args:
        words: words list
        n: length of ngram

    Returns:
        list of ngrams of certain length

    """
    if len(words)<= n:
        print (len(words), '/',n)
        return words
    else:
        ngram_set = set()
        for i in range(len(words)-n+1):
            ngram_set.add(' '.join(words[i:i+n]))
        return list(ngram_set)

def ngram_generation(question):
    """

    Note:
        generate ngrams of a question

    Args:
        question: question in natural language

    Returns:
        list of all possible ngrams

    """
    ngrams_set = set()
    unigrams = question.strip().split(' ')
    filtered_grams = unigrams
    for i in range(1, len(filtered_grams)):
        ngrams_set.update(ngrams(filtered_grams, i))
    return list(ngrams_set)

def ngram_clean(ngram_list, alias):
    """

    Note:
        remove ngrams which is not a label of entities

    Args:
        ngram_list: ngrams generated by a question
        alias: label of entities

    Returns:
        ngrams related to entities

    """
    filter_ngrams = [ngram for ngram in ngram_list if ngram in alias]
    return filter_ngrams

# 
def ngram_vectorisation(ngram2mid, mid2index, ngram_list):
    """

    Note:
        get index list of ngrams

    Args:
        ngram_list
        ngram2mid: map ngram to id
        mid2index: map id to index

    Returns:
        list of index

    """
    mid_list = [ngram2mid[ngram] for ngram in ngram_list if ngram in ngram2mid.keys()]
    mid_list.extend([ngram2mid[ngram.lower()] for ngram in ngram_list if ngram.lower() in ngram2mid.keys()])
    mids = mid2index.keys()
    index_list = [mid2index[mid] for mid in mid_list if mid in mids]
    return index_list

def candidate_generation(f_y, f_y_size, symbol_size, ngram2mid, mid2index, question):
    """

    Note:
        generate candidates with ngrams 

    Args:
        f_y: knowlege base matrice
        f_y_size: 
        ngram2mid: map ngram to id
        mid2index: map id to index
        question: in natural language

    Returns:
        list of candidates

    """
    candidates = list()
    ngram_list = ngram_generation(question)
    ngram_list = ngram_clean(ngram_list, ngram2mid.keys())
    ngram_indexes = ngram_vectorisation(ngram2mid, mid2index, ngram_list)

    if len(ngram_indexes) > 0:
        ngram_tuples = [(1, 0, index) for index in ngram_indexes]

        f_data, f_row, f_col = zip(*ngram_tuples)
        ngram_vector = sparse.csr_matrix((f_data, (f_row, f_col)), shape=(1, symbol_size))
        ngram_vector = ngram_vector.toarray()

        candidates.append(f_y.dot(ngram_vector.transpose()))
        candidates = candidates[0]
        return [index for index in range(len(candidates)) if candidates[index][0] > 0.0]
    else:
        return []